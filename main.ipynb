{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C, PPO\n",
    "from locomotion.robots import robot_config\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_dir = \"./models/PPO\"\n",
    "log_dir = \"./logs\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('locomotion:A1GymEnv-v1', render=False)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "for i in range(1, 100):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f\"{model_dir}/{TIMESTEPS*i}\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darsh\\Desktop\\model_based_locomotion\\venv\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "c:\\Users\\darsh\\Desktop\\model_based_locomotion\\venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     action, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs)\n\u001b[1;32m---> 12\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     14\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\venv\\lib\\site-packages\\gym\\wrappers\\time_limit.py:16\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\envs\\gym_envs\\a1_gym_env.py:27\u001b[0m, in \u001b[0;36mA1GymEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 27\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\envs\\env_wrappers\\trajectory_generator_wrapper_env.py:89\u001b[0m, in \u001b[0;36mTrajectoryGeneratorWrapperEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     84\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAction cannot be None\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m new_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trajectory_generator\u001b[39m.\u001b[39mget_action(\n\u001b[0;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gym_env\u001b[39m.\u001b[39mrobot\u001b[39m.\u001b[39mGetTimeSinceReset(), action)\n\u001b[1;32m---> 89\u001b[0m original_observation, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gym_env\u001b[39m.\u001b[39;49mstep(new_action)\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modify_observation(original_observation), reward, done, _\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\envs\\env_wrappers\\observation_dictionary_to_array_wrapper.py:61\u001b[0m, in \u001b[0;36mObservationDictionaryToArrayWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     52\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Steps the wrapped environment.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m      end indicator.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m   observation_dict, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gym_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     62\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flatten_observation(observation_dict), reward, done, _\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\envs\\locomotion_gym_env.py:317\u001b[0m, in \u001b[0;36mLocomotionGymEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    314\u001b[0m   env_randomizer\u001b[39m.\u001b[39mrandomize_step(\u001b[39mself\u001b[39m)\n\u001b[0;32m    316\u001b[0m \u001b[39m# robot class and put the logics here.\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_robot\u001b[39m.\u001b[39;49mStep(action)\n\u001b[0;32m    319\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_sensors():\n\u001b[0;32m    320\u001b[0m   s\u001b[39m.\u001b[39mon_step(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\robots\\minitaur.py:248\u001b[0m, in \u001b[0;36mMinitaur.Step\u001b[1;34m(self, action, motor_control_mode)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_repeat):\n\u001b[0;32m    247\u001b[0m   proc_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mProcessAction(action, i)\n\u001b[1;32m--> 248\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_StepInternal(proc_action, motor_control_mode)\n\u001b[0;32m    249\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    251\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_action \u001b[39m=\u001b[39m action\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\robots\\minitaur.py:236\u001b[0m, in \u001b[0;36mMinitaur._StepInternal\u001b[1;34m(self, action, motor_control_mode)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_StepInternal\u001b[39m(\u001b[39mself\u001b[39m, action, motor_control_mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 236\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mApplyAction(action, motor_control_mode)\n\u001b[0;32m    237\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pybullet_client\u001b[39m.\u001b[39mstepSimulation()\n\u001b[0;32m    238\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReceiveObservation()\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\robots\\a1.py:429\u001b[0m, in \u001b[0;36mA1.ApplyAction\u001b[1;34m(self, motor_commands, motor_control_mode)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_clip_motor_commands:\n\u001b[0;32m    428\u001b[0m   motor_commands \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ClipMotorCommands(motor_commands)\n\u001b[1;32m--> 429\u001b[0m \u001b[39msuper\u001b[39;49m(A1, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mApplyAction(motor_commands, motor_control_mode)\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\robots\\minitaur.py:938\u001b[0m, in \u001b[0;36mMinitaur.ApplyAction\u001b[1;34m(self, motor_commands, motor_control_mode)\u001b[0m\n\u001b[0;32m    936\u001b[0m     motor_ids\u001b[39m.\u001b[39mappend(motor_id)\n\u001b[0;32m    937\u001b[0m     motor_torques\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 938\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_SetMotorTorqueByIds(motor_ids, motor_torques)\n",
      "File \u001b[1;32mc:\\Users\\darsh\\Desktop\\model_based_locomotion\\locomotion\\robots\\minitaur.py:494\u001b[0m, in \u001b[0;36mMinitaur._SetMotorTorqueByIds\u001b[1;34m(self, motor_ids, torques)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_SetMotorTorqueByIds\u001b[39m(\u001b[39mself\u001b[39m, motor_ids, torques):\n\u001b[1;32m--> 494\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pybullet_client\u001b[39m.\u001b[39;49msetJointMotorControlArray(\n\u001b[0;32m    495\u001b[0m       bodyIndex\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquadruped,\n\u001b[0;32m    496\u001b[0m       jointIndices\u001b[39m=\u001b[39;49mmotor_ids,\n\u001b[0;32m    497\u001b[0m       controlMode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pybullet_client\u001b[39m.\u001b[39;49mTORQUE_CONTROL,\n\u001b[0;32m    498\u001b[0m       forces\u001b[39m=\u001b[39;49mtorques)\n",
      "\u001b[1;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "env = gym.make('locomotion:A1GymEnv-v1', render=True)\n",
    "env.reset()\n",
    "\n",
    "# model_pth = f\"{model_dir}/850000.zip\"\n",
    "model_pth = f\"./artifacts/swimmer.zip\"\n",
    "model = A2C.load(model_pth, env)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while True:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('locomotion:A1GymEnv-v1', render=True)\n",
    "obs = env.reset()\n",
    "\n",
    "done = False\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    np.save(\"./action.npy\", action)\n",
    "    np.save(\"./observation.npy\", obs)\n",
    "    break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('locomotion:A1GymEnv-v1', render=True)\n",
    "obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
