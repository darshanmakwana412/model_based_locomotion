{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dynamics import *\n",
    "import matplotlib.pyplot as plt\n",
    "from locomotion.envs.gym_envs import A1GymEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the robot environment\n",
    "env = A1GymEnv(action_limit=(0.75, 0.75, 0.75), render=True, on_rack=False, lateralFriction=0.7)\n",
    "device = \"cpu\"\n",
    "num_samples = 1000\n",
    "horizon = 5\n",
    "# Init the NN dynamics model\n",
    "dynamics = Dynamics(\n",
    "    n_in=31,\n",
    "    n_hidden=500,\n",
    "    n_out=19,\n",
    "    depth=3\n",
    ")\n",
    "dynamics.load_state_dict(torch.load(\"./logs/dynamics.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_at(env):\n",
    "    at = env.action_space.sample()\n",
    "    at = np.concatenate([at[:6], [0]*6])\n",
    "    return at\n",
    "\n",
    "\n",
    "def MPC(dynamics, s0, env, horizon, num_samples, device):\n",
    "    st = np.array([s0 for _ in range(num_samples)])\n",
    "    a0 = np.array([get_rand_at(env) for _ in range(num_samples)])\n",
    "    rt = torch.zeros(num_samples)\n",
    "    for t in range(horizon):\n",
    "\n",
    "        if t != 0:\n",
    "            at = np.array([get_rand_at(env) for _ in range(num_samples)])\n",
    "        else:\n",
    "            at = a0\n",
    "\n",
    "        X = np.concatenate((st, at), axis=1)\n",
    "        X = torch.from_numpy(X).to(device).float()\n",
    "\n",
    "        st_1 = dynamics(X).cpu().detach().numpy()\n",
    "        rt += -(st_1[:, 0] - st[:, 0])\n",
    "        st = st_1\n",
    "    idx = np.argmax(rt)\n",
    "    return a0[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics.eval()\n",
    "dynamics = dynamics.to(device)\n",
    "rewards = [0]\n",
    "\n",
    "st = env.reset()\n",
    "for _ in tqdm(range(1000)):\n",
    "    at = MPC(\n",
    "        dynamics,\n",
    "        st,\n",
    "        env,\n",
    "        1,\n",
    "        5000,\n",
    "        device=device\n",
    "    )\n",
    "    st_1, reward, done, info = env.step(at)\n",
    "    rewards.append(rewards[-1] + reward)\n",
    "\n",
    "    st = st_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "rt = np.array(rewards) * -1.0\n",
    "plt.plot(rt)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative Reward\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
